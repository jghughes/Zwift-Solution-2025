"""
This monster tool is not used directly in the Brute production
pipeline. The output file from this tool08 is the input file 
for tool12.py, which is also not used.

I wrote this tool to investigate and compare and find
correlations between raw and synthetic power data and Zwift's
inscrutable zFTP metric. I am not remotely concerned with pull power
curve modelling in this tool, only one-hour power curve modelling.
Go to tool12.py to learn all about how I used the data generated by
this tool to experiment extensively with linear regression using
sklearn, scipy, and matplotlib. There I also analyse the correlations
between zFTP and 40-minute power, 60-minute power, and 90-day best
power data.

The monster tool incorporates all the tested and proven steps in the
previous tools. It repeats them, operating on the thousands of files 
scraped by DaveK from Zwift, ZwiftPower, and ZwiftRacingApp.
It performs automated power curve modeling and dataset generation for
the entire set of Zwift riders using their 90-day best power data. In
the April 2025 dataset there were 1,515 riders in the club, and this
tool processes all of them in one go. It takes a few minutes to run.
After filtering out riders with no power data, or too few datapoints,
or curves who do not satisfy the acceptable minimum r-squared of 0.90
for the one-hour power curve, the number reduces to 360. These riders
are regarded as having curve fits of sufficiently high fidelity for
meaningful regression analysis of the factors that contribute to zFTP,
which Zwift asserts is comparable to 40 - 60 minute power. The
comparable figures for July 2025 are 1,553 riders in the club and 263
riders with high-fidelity models.

The data generated by this tool is consolidated along with all the raw
power data in the special purpose RegressionModellingItem for each
rider.

Note that this is the earliest tool in which we see the heap powerful
RepositoryForScrapedDataFromDaveK class in action. Its .populate() method is
used in the production pipeline in Brute to load all the Zwift
profiles, ZwiftPower profiles, and ZwiftPower 90-day power data from
all files for all riders in one fell swoop and output a JSON dictionary
of ZsunItems that can be copied manually in the data folder of
Zsun01 in production. The repository has been extensively debugged.

The script performs the following steps:
- Configures logging for the application.
- Loads Zwift profile and best power data for all available riders from
  specified directories.
- For each rider, validates the data and extracts power-duration data
  for three modeling zones: critical power (CP & W'), TTT pull power,
  and one-hour (FTP) power.
- Fits mathematical models to each zone using curve fitting techniques
  to estimate physiological parameters such as critical power,
  anaerobic work capacity, and power curve coefficients.
- Aggregates the modeling results and merges them with rider profile
  data into a single dataset.
- Writes the merged dataset to an Excel file for further analysis.
- Identifies riders with high-fidelity model fits and generates a
  separate dataset for these riders, saving it in both Excel and JSON
  formats for use in advanced machine learning applications.

This tool demonstrates large-scale data loading, validation, model
fitting, aggregation, and export for regression analysis. Note the use
of Pandas for creating, merging, and writing tabular data as .csv files
for Excel.
"""
from typing import DefaultDict
from collections import defaultdict
from datetime import datetime
import numpy as np
from repository_of_scraped_riders import read_zwift_files, read_zwiftpower_graph_watts_files
from handy_utilities import write_json_dict_of_regressionmodellingItem
from critical_power import do_curve_fit_with_cp_w_prime_model, do_curve_fit_with_decay_model, decay_model_numpy 
from jgh_read_write import write_pandas_dataframe_as_xlsx
from zsun_watts_properties_item import ZsunWattsItem
from zsun_rider_item import ZsunItem
from repository_of_scraped_riders import RepositoryForScrapedDataFromDaveK
from computation_classes import CurveFittingResultItem
from regression_modelling_item import RegressionModellingItem
from dirpaths import ZWIFT_DIRPATH, ZWIFTPOWER_GRAPHS_DIRPATH, ZWIFTRACINGAPP_DIRPATH, ZWIFTPOWER_DIRPATH
import logging
logger = logging.getLogger(__name__)

def main():
 
    dict_of_zwift_profiles_for_everybody = read_zwift_files(None, ZWIFT_DIRPATH)

    dict_of_zsun_watts_graphs_for_everybody = read_zwiftpower_graph_watts_files(None, ZWIFTPOWER_GRAPHS_DIRPATH)

    logger.info(f"Successfully read, validated, and loaded {len(dict_of_zsun_watts_graphs_for_everybody)} 90-day best graphs from ZwiftPower files in:- \nDir : {ZWIFTPOWER_GRAPHS_DIRPATH}\n\n")

    # create a list of zwiftrider objects from the raw data

    total_count = 0
    skipped_modelling_count = 0
    valid_count = 0
    count_of_riders_with_high_fidelity_models = 0
    count_of_riders_with_low_fidelity_models = 0


    zwiftIds_with_high_fidelity : list[str] = []
    zwiftids_with_low_fidelity : list[str] = []

    power_curves_for_everybody : dict[str, CurveFittingResultItem] = {}

    for my_zwiftID, my_zsunwattsgraphitem in dict_of_zsun_watts_graphs_for_everybody.items():

        my_zsunwattsgraphitem.zwift_id = my_zwiftID

        total_count += 1

        # skip riders with no data
        datapoints = my_zsunwattsgraphitem.export_all_x_y_ordinates()

        if not datapoints:
            logger.warning(f"ZwiftID {my_zsunwattsgraphitem.zwift_id} has no datapoints")
            skipped_modelling_count += 1
            continue

        #skip riders where all the datapoints are zero
        if all(value == 0 for value in datapoints.values()):
            logger.warning(f"ZwiftID {my_zsunwattsgraphitem.zwift_id} has empty data")
            skipped_modelling_count += 1
            continue
        
        # succeess - on we go

        # obtain raw xy data for the various ranges - critical_power, pull, and ftp

        raw_xy_data_cp = my_zsunwattsgraphitem.export_x_y_ordinates_for_cp_w_prime_modelling()
        raw_xy_data_pull = my_zsunwattsgraphitem.export_x_y_ordinates_for_pull_zone_modelling()
        raw_xy_data_one_hour = my_zsunwattsgraphitem.export_x_y_ordinates_for_one_hour_zone_modelling()

        # skip riders where any of the three datasets contain less than 5 points
        if len(raw_xy_data_cp) < 5 or len(raw_xy_data_pull) < 5 or len(raw_xy_data_one_hour) < 5:
            logger.warning(f"ZwiftID {my_zsunwattsgraphitem.zwift_id} has insufficient data for curve fitting")
            skipped_modelling_count += 1
            continue

        # do power modelling
    
        critical_power, anaerobic_work_capacity, _, _, _  = do_curve_fit_with_cp_w_prime_model(raw_xy_data_cp)
        coefficient_pull, exponent_pull, r_squared_pull, _, _ = do_curve_fit_with_decay_model(raw_xy_data_pull)
        coefficient_one_hour, exponent_one_hour, r_squared_one_hour, _, _ = do_curve_fit_with_decay_model(raw_xy_data_one_hour)

        pull_short = decay_model_numpy(np.array([300]), coefficient_pull, exponent_pull)
        pull_medium = decay_model_numpy(np.array([600]), coefficient_pull, exponent_pull)
        pull_long = decay_model_numpy(np.array([1800]), coefficient_pull, exponent_pull)
        one_hour = decay_model_numpy(np.array([60*60]), coefficient_one_hour, exponent_one_hour)

        # load results into answer

        #load results into a dataclass
        curve = CurveFittingResultItem(
            zwift_id=my_zwiftID,
            one_hour_curve_coefficient=coefficient_one_hour,
            one_hour_curve_exponent=exponent_one_hour,
            one_hour_curve_r_squared=round(r_squared_one_hour, 2),
            TTT_pull_curve_coefficient=coefficient_pull,
            TTT_pull_curve_exponent=exponent_pull,
            TTT_pull_curve_r_squared=round(r_squared_pull, 2),
            CP=round(critical_power),
            AWC=round((anaerobic_work_capacity / 1_000.0), 1),
            when_curves_fitted=datetime.now().isoformat()  # Add timestamp
        )
        power_curves_for_everybody[str(my_zsunwattsgraphitem.zwift_id)] = curve

        # log results for my_zsunwattsgraphitem

        summary_cp_w_prime  =  f"Zsun CP = {round(critical_power)}W  AWC = {round(anaerobic_work_capacity/1_000)}kJ"
        summary_pull = f"TTT pull power (30 - 60 - 120 seconds) = {round(pull_short[0])} - {round(pull_medium[0])} - {round(pull_long[0])}W"
        summary_one_hour = f"One hour power = {round(one_hour[0])}W"

        logger.info(f"\n{summary_cp_w_prime}")
        logger.info(f"{summary_pull}")
        logger.info(f"{summary_one_hour}")


        if r_squared_one_hour >= MINIMUM_REQUIRED_R_SQUARED_FIT_FOR_ONE_HOUR_POWER_CURVE:
            zwiftIds_with_high_fidelity.append(my_zsunwattsgraphitem.zwift_id)
            count_of_riders_with_high_fidelity_models += 1
        else:
            zwiftids_with_low_fidelity.append(my_zsunwattsgraphitem.zwift_id)
            count_of_riders_with_low_fidelity_models += 1

        valid_count += 1

    modelled_count = total_count - skipped_modelling_count

    logger.info(f"\nTotal riders on ZwiftPower from DaveK: {total_count}  Insufficient data : {skipped_modelling_count}  Modelled count: {modelled_count}\n")

    logger.info(f"Riders with excellent TTT pull curve fits [r_squared > {MINIMUM_REQUIRED_R_SQUARED_FIT_FOR_ONE_HOUR_POWER_CURVE}] : {count_of_riders_with_high_fidelity_models} ({round(100.0*count_of_riders_with_high_fidelity_models/modelled_count)}%)")

    # for  zwiftIds_with_high_fidelity, write out the zwiftID, name, critical_power, and r_squared_cp. sorted by name
    zwiftIds_with_high_fidelity.sort(key=lambda x: x)
    for zwift_id in zwiftIds_with_high_fidelity:
        logger.info(f"ZwiftID {zwift_id} {dict_of_zwift_profiles_for_everybody[zwift_id].first_name} {dict_of_zwift_profiles_for_everybody[zwift_id].last_name}")
        # betel = get_test_ZsunDTO(zwift_id)
        # logger.info(f"ZwiftID {zwift_id} : {betel.name}")

    logger.info(f"\nTotal in sample : {total_count} Total valid: {valid_count} Total excellent one hr r2: {count_of_riders_with_high_fidelity_models} % excellent/valid: {round(count_of_riders_with_high_fidelity_models *100/valid_count)}%\n")

    import pandas as pd
    from dataclasses import asdict

    # Create the first DataFrame from dict_of_zwift_profiles_for_everybody
    df1 = pd.DataFrame([asdict(value) for value in dict_of_zwift_profiles_for_everybody.values()])

    # Create the second DataFrame from power_curves_for_everybody
    df2 = pd.DataFrame([asdict(value) for value in power_curves_for_everybody.values()])

    # Merge the two DataFrames on the identifier columns
    merged_df = pd.merge(df1, df2, left_on="zwift_id", right_on="zwift_id", suffixes=('_profile', '_power'))

    # write to excel
    write_pandas_dataframe_as_xlsx(merged_df, CURVE_FITTING_FILENAME_FOR_EXCEL, OUTPUT_DIRPATH)
    logger.info(f"\nSaved {len(merged_df)} power curve fitting results to: {OUTPUT_DIRPATH + CURVE_FITTING_FILENAME_FOR_EXCEL}\n")

    # map zwiftIds_with_high_fidelity into a list of custom objects and save to json file for use for sophisicated machine learning to determine zFTP

    repository : RepositoryForScrapedDataFromDaveK = RepositoryForScrapedDataFromDaveK()

    # AOK. Restart from the beginning with concise dataload. HEAP POWERFUL
    repository.populate_repository(None, ZWIFT_DIRPATH, ZWIFTRACINGAPP_DIRPATH, ZWIFTPOWER_DIRPATH, ZWIFTPOWER_GRAPHS_DIRPATH) 
    dict_of_ZsunItems : defaultdict[str, ZsunItem] = repository.get_dict_of_ZsunItem(zwiftIds_with_high_fidelity)
    dict_of_zp_90day_graph_watts : DefaultDict[str,ZsunWattsItem] = repository.get_dict_of_ZsunWattsItem(zwiftIds_with_high_fidelity)
    
    
    dict_of_riders_with_high_fidelity : DefaultDict[str, RegressionModellingItem] = defaultdict(RegressionModellingItem)

    for ID in zwiftIds_with_high_fidelity:
        zwift = dict_of_zwift_profiles_for_everybody[ID]
        zsun = dict_of_ZsunItems[ID]
        zp_90day_best = dict_of_zp_90day_graph_watts[ID]
        dict_of_riders_with_high_fidelity[ID] = RegressionModellingItem(
            zwift_id                   = ID,
            name                       = zsun.name,
            gender                     = zsun.gender,
            weight_kg                  = zsun.weight_kg,
            height_cm                  = zsun.height_cm,
            age_years                  = zsun.age_years,
            zwift_zrs                  = zsun.zwift_zrs,
            zwift_cat                  = zsun.zwift_cat,
            zwift_ftp                  = zwift.ftp,
            zsun_one_hour_watts        = round(zsun.get_one_hour_watts()),
            zsun_40_minute_watts       = round(zsun.get_n_second_watts(2400)),
            zwiftracingapp_zpFTP       = zsun.zwiftracingapp_zpFTP,
            zwiftracingapp_score       = zsun.zwiftracingapp_score,
            zwiftracingapp_cat_num     = zsun.zwiftracingapp_cat_num,
            zwiftracingapp_cat_name    = zsun.zwiftracingapp_cat_name,
            bp_5                       = zp_90day_best.bp_5,
            bp_15                      = zp_90day_best.bp_15,
            bp_30                      = zp_90day_best.bp_30,
            bp_60                      = zp_90day_best.bp_60,
            bp_180                     = zp_90day_best.bp_180,
            bp_300                     = zp_90day_best.bp_300,
            bp_600                     = zp_90day_best.bp_600,
            bp_720                     = zp_90day_best.bp_720,
            bp_900                     = zp_90day_best.bp_900,
            bp_1200                    = zp_90day_best.bp_1200,
            bp_1800                    = zp_90day_best.bp_1800,
            bp_2400                    = zp_90day_best.bp_2400,
            zsun_one_hour_curve_coefficient = zsun.zsun_one_hour_curve_coefficient,
            zsun_one_hour_curve_exponent = zsun.zsun_one_hour_curve_exponent
        )

    # Create the third DataFrame from dict_of_riders_with_high_fidelity
    riders = dict_of_riders_with_high_fidelity.values()
    df3 = pd.DataFrame([asdict(modelTrainingItem) for modelTrainingItem in riders])

    write_pandas_dataframe_as_xlsx(df3, REGRESSION_FILENAME_EXCEL, OUTPUT_DIRPATH)
    logger.info(f"\nSaved {len(df3)} correlation data-set items to: {OUTPUT_DIRPATH}{REGRESSION_FILENAME_EXCEL}\n")

    write_json_dict_of_regressionmodellingItem(dict_of_riders_with_high_fidelity, REGRESSION_FILENAME_JSON, OUTPUT_DIRPATH)

if __name__ == "__main__":
    # configure root logging since this is the entry point
    from jgh_logging import jgh_configure_logging
    jgh_configure_logging("appsettings.json")
    logging.getLogger("numba").setLevel(logging.ERROR) # numba is noisy at INFO level

    MINIMUM_REQUIRED_R_SQUARED_FIT_FOR_ONE_HOUR_POWER_CURVE = .90

    # output destination for results - the input ingested by tool12.py - make sure the filenames match each other
    REGRESSION_FILENAME_EXCEL = "dataset_for_linear_regression_investigations_using_sklearn.xlsx"
    REGRESSION_FILENAME_JSON = "dataset_for_linear_regression_investigations_using_sklearn.json"
    CURVE_FITTING_FILENAME_FOR_EXCEL = "power_curve_fitting_results_for_club_by_jgh.xlsx"
    OUTPUT_DIRPATH = "C:/Users/johng/holding_pen/StuffForZsun/!StuffFromDaveK_byJgh/zsun_everything_2025-07-08/"

    main()


